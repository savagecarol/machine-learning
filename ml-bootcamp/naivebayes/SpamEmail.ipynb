{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ham',\n",
       "        'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "        nan, nan, nan],\n",
       "       ['ham', 'Ok lar... Joking wif u oni...', nan, nan, nan],\n",
       "       ['spam',\n",
       "        \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "        nan, nan, nan],\n",
       "       ...,\n",
       "       ['ham',\n",
       "        'Pity, * was in mood for that. So...any other suggestions?', nan,\n",
       "        nan, nan],\n",
       "       ['ham',\n",
       "        \"The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\",\n",
       "        nan, nan, nan],\n",
       "       ['ham', 'Rofl. Its true to its name', nan, nan, nan]], dtype=object)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 1]\n",
    "y = data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5572,), (5572,))"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "        'Ok lar... Joking wif u oni...',\n",
       "        \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "        ..., 'Pity, * was in mood for that. So...any other suggestions?',\n",
       "        \"The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\",\n",
       "        'Rofl. Its true to its name'], dtype=object),\n",
       " array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype=object))"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "sw = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a clean document\n",
    "def getDoc(document):\n",
    "    d = []\n",
    "    for doc in document:\n",
    "        d.append(getStem(doc))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_doc = getDoc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though',\n",
       " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send å 1 50 rcv',\n",
       " 'even brother like speak treat like aid patent',\n",
       " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
       " 'winner valu network custom select receivea å 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
       " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030']"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_doc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my vocab\n",
    "vc = cv.fit_transform(stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vc.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977705274605764"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "    Hi Kunal,\n",
    "We invite you to participate in MishMash - India’s largest online diversity hackathon. \n",
    "The hackathon is a Skillenza initiative and sponsored by Microsoft, Unity, Unilever, Gojek, Rocketium and Jharkhand Government. \n",
    "We have a special theme for you - Deep Tech/Machine Learning - sponsored by Unilever, which will be perfect for you.\n",
    "    \"\"\",\n",
    "    \"\"\"Join us today at 12:00 PM ET / 16:00 UTC for a Red Hat DevNation tech talk on AWS Lambda and serverless Java with Bill Burke.\n",
    "Have you ever tried Java on AWS Lambda but found that the cold-start latency and memory usage were far too high? \n",
    "In this session, we will show how we optimized Java for serverless applications by leveraging GraalVM with Quarkus to \n",
    "provide both supersonic startup speed and a subatomic memory footprint.\"\"\",\n",
    "\n",
    "    \"\"\"We really appreciate your interest and wanted to let you know that we have received your application.\n",
    "There is strong competition for jobs at Intel, and we receive many applications. As a result, it may take some time to get back to you.\n",
    "Whether or not this position ends up being a fit, we will keep your information per data retention policies, \n",
    "so we can contact you for other positions that align to your experience and skill set.\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(messages):\n",
    "    d = getDoc(messages)\n",
    "    # dont do fit_transform!! it will create new vocab.\n",
    "    return cv.transform(d)\n",
    "\n",
    "messages = prepare(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(messages)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laugh'"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('laughing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_picke','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_picke','rb') as f:\n",
    "    model1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq=model1.predict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getStem(review):\n",
    "#     review = review.lower()\n",
    "#     tokens = tokenizer.tokenize(review) # breaking into small words\n",
    "#     removed_stopwords = [w for w in tokens if w not in sw]\n",
    "#     stemmed_words = [ps.stem(token) for token in removed_stopwords]\n",
    "#     clean_review = ' '.join(stemmed_words)\n",
    "#     return clean_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>\"tweet\"</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717464912415600640</td>\n",
       "      <td>\"ياثارات الحسين\"\\n.\\nالى متى تكف المجزرة الظال...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>794412494077001729</td>\n",
       "      <td>mujahids dont die but in numbers we multiply i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563458330019258369</td>\n",
       "      <td>RT @AbuhAlam: @InviteToIslam well, it was \"whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665946450124103680</td>\n",
       "      <td>#دولة_الخلافة \\n#ولاية_الفرات\\n #عاجل\\nإعطاب ه...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>734350218314063872</td>\n",
       "      <td>RT @K_Dergi3: Suhne Fatihi\\nŞehid Şeyh Ebu Mal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>711146574399852544</td>\n",
       "      <td>@CDIMI landen of mensen moeten geen kopie word...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>565480087584518145</td>\n",
       "      <td>RT @DailyNewsEgypt: Canadian PM calls for Moha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>561897149487538176</td>\n",
       "      <td>@Anaminona thanks.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>663842492094259200</td>\n",
       "      <td>When I told them they are denigrating themselv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>733367668984680448</td>\n",
       "      <td>Wow, now this is interesting. https://t.co/sPM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                            \"tweet\"  \\\n",
       "0   717464912415600640  \"ياثارات الحسين\"\\n.\\nالى متى تكف المجزرة الظال...   \n",
       "1   794412494077001729  mujahids dont die but in numbers we multiply i...   \n",
       "2   563458330019258369  RT @AbuhAlam: @InviteToIslam well, it was \"whe...   \n",
       "3   665946450124103680  #دولة_الخلافة \\n#ولاية_الفرات\\n #عاجل\\nإعطاب ه...   \n",
       "4   734350218314063872  RT @K_Dergi3: Suhne Fatihi\\nŞehid Şeyh Ebu Mal...   \n",
       "..                 ...                                                ...   \n",
       "95  711146574399852544  @CDIMI landen of mensen moeten geen kopie word...   \n",
       "96  565480087584518145  RT @DailyNewsEgypt: Canadian PM calls for Moha...   \n",
       "97  561897149487538176                                 @Anaminona thanks.   \n",
       "98  663842492094259200  When I told them they are denigrating themselv...   \n",
       "99  733367668984680448  Wow, now this is interesting. https://t.co/sPM...   \n",
       "\n",
       "    class  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "..    ...  \n",
       "95      0  \n",
       "96      0  \n",
       "97      0  \n",
       "98      0  \n",
       "99      0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[:, 2]\n",
    "X = data[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in Y:\n",
    "    if i == 1:\n",
    "        y.append(\"Yes\")\n",
    "    else:\n",
    "        y.append(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStem(review):\n",
    "    review = review.lower()\n",
    "    tokens = tokenizer.tokenize(review) # breaking into small words\n",
    "    removed_stopwords = [w for w in tokens if w not in sw]   \n",
    "    \n",
    "    correct = []\n",
    "    for ch in removed_stopwords:\n",
    "        if ('\\u0600' <= ch <= '\\u06FF' or\n",
    "    '\\u0750' <= ch <= '\\u077F' or\n",
    "    '\\u08A0' <= ch <= '\\u08FF' or\n",
    "  '\\uFB50' <= ch <= '\\uFDFF' or\n",
    "    '\\uFE70' <= ch <= '\\uFEFF' or\n",
    "    '\\U00010E60' <= ch <= '\\U00010E7F'): \n",
    "            pass\n",
    "        else:\n",
    "            correct.append(ch)\n",
    "    stemmed_words = [ps.stem(token) for token in correct]\n",
    "    clean_review = ' '.join(stemmed_words)\n",
    "    return clean_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a clean document\n",
    "def getDoc(document):\n",
    "    d = []\n",
    "    for doc in document:\n",
    "        d.append(getStem(doc))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_doc = getDoc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http co teteqc258u',\n",
       " 'mujahid dont die number multipli insha allah oh allah enrich us ikhla taqwa ridaka rahmatika',\n",
       " 'rt abuhalam invitetoislam well coconut tri blend kuffar',\n",
       " 'http co vy740vv27r http co b5zgem1exa',\n",
       " 'rt k_dergi3 suhn fatihi şehid şeyh ebu malik et temimi allah kabul etsin suhn ameliyesi 6 http co t0p8ewyggo',\n",
       " 'rt azamhikma http co f8vxjpl83j',\n",
       " 'iloveafrica_ waleedkona http co xh3qlowpb2',\n",
       " 'shall come upon peopl time wherein shall believ go ash shaam 1',\n",
       " 'spanishkash1 iqy007 http co laaysykm5',\n",
       " 'rt bsmtlolo1 http co wsfirbmwi',\n",
       " 'aqap yemen lenient rule neglect fall short limit set allah w',\n",
       " 'rt mecomuhammad made logo invitetoislam sure follow account great islam remind http co opqda24wxz',\n",
       " 'alhamdulilaah 3ala kulli 7aal sujudshukr allah ya ikhwa bless day rememb brother dua qiyam layl',\n",
       " 'rt qatada_92',\n",
       " 'asd1410 dzdzdz32111 hino38 kasimf',\n",
       " 'rt freefalasteen firebomb pig head thrown mosqu anti muslim attack increas pari shoot http co 9txzz3kwji',\n",
       " 'democraci westbrook hypocrit russia mainstream media cnn bbcqt http co zmaqutafz8',\n",
       " 'close',\n",
       " 'rt eselim268 vallahi bu allah svt bizler ikramidir ganimet biiznillah rabbim islam devletin yeni fetihl nasib etsin insallah http co lanynjuzd3',\n",
       " 'http co 8n66wh5yfh',\n",
       " 'rt j_latkia_n',\n",
       " 'medialiesalway tortur detail public time issu bring whole crew see 1 kashmiri mug 2 2',\n",
       " 'just_bstrang ameen',\n",
       " 'lol murtadeen start wheel http co jwb5uh0cwc',\n",
       " 'lieadar_1599',\n",
       " 'rt syamilajasmln penghancuran kendaraan hummer mobil milit pasukan irak di jabal makhool sebelah barat laut dari baiji http co wgtc3jtk04',\n",
       " '_antiwish_ somali_abu u block anoth akh abu',\n",
       " 'rt muslim3un dhikr remembr allāh whip shaytān hurt tortur like whip rod ibn al qayyim',\n",
       " 'follow spread australiabu munthir555 ivanmohamedi abu_halif',\n",
       " 'plsdontgetrud get blanket',\n",
       " 'rt inghuimass j0o_0ori',\n",
       " 'rt truth_haqq1 forgotten brother shamiwit taken hostag pagan may allah swt hasten releas http co c2wf5cb7xb',\n",
       " 'rt shamianalyst hom provinc hadd punish carri spi murtad apost http co ggbvuoij1p http co 7grrsjuvjo',\n",
       " 'ummussamah11 see arab subtitl lol',\n",
       " 'rt el_turkii1 abdiwaahid spread new account akhi jazak allahu kheyr',\n",
       " 'rt islamforever17 pari attack 12 men die barbar gaza fire 4000 civilian massacr act self defenc achiev peac unjust world',\n",
       " 'rt didyouknowmedia franc found nation crusad first crusad nation histori parisattack http co v3gffjvzat',\n",
       " 'veilleurv avec jan surement mai avec l ei c est de l intox un de leur dirig été reçu non officiel à washington',\n",
       " 'break confirm inde car bomb explod masqaan mani kill hasbuna allah aleppo',\n",
       " 'rt benjaminnorton us militari abus afghan prison correct word tortur nytim http co ix5legqwch http co p8kiysui31',\n",
       " 'rt nikorea_ dont fight die fight liveee',\n",
       " 'q develop yemen salman bin abdul aziz assum rule saudi question interest http co ra9qghgtb',\n",
       " 'umm_nusaybah_5 haven earth belong even belong properti right',\n",
       " 'rt conflict video first moment gunfir heard rock concert pari bataclan concert hall israelhatzolah http co z2i9k59tx7',\n",
       " 'almost burst furi everi time group cast therein keeper ask warner come 67 8',\n",
       " 'mohib_ayubi iraqnow9 inshallah brother may armi haq defeat armi batel falsehood may allah subhanu aid us victori',\n",
       " 'banyak bertebaran hastag prayfornic dari media anshar pahami ya gae itu cuma satir sarkasm nyindir udah gitu aja',\n",
       " 'rt hasss7ad http co vk0x3kyv26',\n",
       " 'rt rt_com break bomb squad summon suspici truck park outsid nice http co 36d41lw5i6 http co o8f9srywyg niceattack',\n",
       " 'gibboxxx subhanallah muslim',\n",
       " 'iammeeesh saadgif',\n",
       " 'german cabinet okay draft law prevent jihadist recruit http co u57wj0wuzi cpvt',\n",
       " 'new video messag islamicst yemen isi strive soul wilayat adan abyan http co nzv6oqbmnk',\n",
       " 'israel launch air strike gaza milit rocket salvo http co 6aun4plgak cpvt',\n",
       " 'daeshbag throw rock women women throw rock daeshbag dawla dawlah caliph islamicst raqqa http co apynxxus6',\n",
       " 'rt ayibeebe allah want good someon afflict trial sahih bukhari patient http co 0iekuxsuku',\n",
       " 'nighttid tri shift news attent pari',\n",
       " 'syrianfalcon11 ajal mhmoud_syria',\n",
       " '100 000 peopl follow pope instagram account hour open http co frowlkmd63 http co zeum5geebk',\n",
       " 'rt wsj investig perplex egyptair flight 804 autom warn system http co o5lu0dzyx5',\n",
       " '',\n",
       " 'http co trblugwwim via rudaw_arab',\n",
       " 'pretfl dagelijks kost ik moet maar een beginnen aan mijn tweed boek',\n",
       " '',\n",
       " 'rt moscow_ghost isi show fighter india hom syria tracterror http co iix0p3abgj http co xmv07ikqak http co olj5hnh596',\n",
       " 'ockham bron kraam je gewoon onzin op social media voor een like',\n",
       " 'rt africawrit http co cj1gwzsy3n',\n",
       " 'rnicholsonjr obama word com man',\n",
       " 'abual9acem dallicious050 ejmalrai saraewilliam aamer045 qstn touger diff state diff polici deal other refus',\n",
       " 'owenjones84 kind countri would like live http co dtxhehchp2',\n",
       " 'rt defencehq iraqi kurdish forc continu op around mosul uk team erbil continu train peshmerga http co ekb6ra4ab',\n",
       " '',\n",
       " 'chrisklomp deskundig easi dezer dagen de media bepaalt dat niet je ergen ervar hebt',\n",
       " 'volgen de nva moet on belgisch koningshui er ander gaan uitzien daarme bedoelen ze de onafhankelijk van vlaanderen gestart',\n",
       " '',\n",
       " 'anoth hdp observ attack akp member urfa osman baydemir consol hospit http co 3zlnzt9kwj http co x0sxnunvao',\n",
       " 'politi belangrijk strijder gedood cairo http co dbvjyesibg via nrc',\n",
       " 'rt jake_hanrahan erdogan host g20 moham rasool sit behind bar without trial 80th day today freerasool http co wrbli50d45',\n",
       " 'zorgen om samenwerk bokoharam met http co rbzxfap8c9',\n",
       " 'rt pirehelo kurdish class east kurdistan 1979 kurd free iranian tyranni could educ kurdish last yea http co 3pymkvhoi4',\n",
       " '10 iraqi forc kill injur vbi op outskirt villag salahiyyah south mosul',\n",
       " '3 ibrahim appar dead 1 suicid bomber manhunt salah said logist man attack fate 3rd bilal unclear',\n",
       " 'rt xn49r http co evutf9m5ar',\n",
       " 'trac incid report mosul offens islamicst attack shirqat south mosul http co 5gnrhomzl9',\n",
       " 'gaza milit condemn egypt brand hama terror group http co lnwgs5e4tv cpvt',\n",
       " 'sponsor orphan ramadan 100 donat go orphan donat today http co 0q9jwdetrz http co rejthailrp',\n",
       " 'http co cydv1nj9ei',\n",
       " 'de explosi te horen tijden de voetbalwedstrijd frankrijk duitsland pari http co 3ssxwmtmuq',\n",
       " 'rt ap break u back kurdish led syrian forc announc start campaign retak islam state group capit raqqa',\n",
       " 'dawlat_islam6 content remov',\n",
       " 'barackobama first us presid 88 year visit cuba visit far http co wzokmlorh1',\n",
       " 'russia putin say hope next round syria talk bring resolut http co wf1oel0ixv cpvt',\n",
       " 'okay watvoor broeder waar zijn ze inshaa allah http co 4xbz0pkwt8',\n",
       " 'rt aleeharissi ff afp colleagu beirut baghdad sarahussein gebeilym layalarah moussrana roba_h ammar_afp wgdunlop mojobaghdad',\n",
       " 'readdanwrit year',\n",
       " 'cdimi landen mensen moeten geen kopi worden van elkaar vind ik',\n",
       " 'rt dailynewsegypt canadian pm call moham fahmi immedi releas http co 58oc7hocc4 http co wzjgdynjw',\n",
       " 'anaminona thank',\n",
       " 'told denigr work maffia said choic',\n",
       " 'wow interest http co spmvfjdxpt']"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = cv.fit_transform(stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train)\n",
    "model1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "    Hi Kunal,\n",
    "We invite you to participate in MishMash - India’s largest online diversity hackathon. \n",
    "The hackathon is a Skillenza initiative and sponsored by Microsoft, Unity, Unilever, Gojek, Rocketium and Jharkhand Government. \n",
    "We have a special theme for you - Deep Tech/Machine Learning - sponsored by Unilever, which will be perfect for you.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi kunal invit particip mishmash india largest onlin divers hackathon hackathon skillenza initi sponsor microsoft uniti unilev gojek rocketium jharkhand govern special theme deep tech machin learn sponsor unilev perfect']\n"
     ]
    }
   ],
   "source": [
    "def prepare(messages):\n",
    "    d = getDoc(messages)\n",
    "    # dont do fit_transform!! it will create new vocab.\n",
    "    return cv.transform(d)\n",
    "\n",
    "messages = prepare(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x724 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
